{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook : Exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from si.data.dataset import Dataset\n",
    "from si.feature_selection.select_percentile import SelectPercentile\n",
    "from si.statistics.f_classification import f_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Class 1\n",
    "\n",
    "## Exercise 1: NumPy array Indexing/Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCICIO 1\n",
    "\n",
    "#1.1 - Loading the \"iris.csv\" using the appropriate method\n",
    "\n",
    "path = r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris.csv'\n",
    "\n",
    "dataset = read_csv(path,features=True, label=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penultimate Independent Variable:\n",
      "[1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n",
      " 1.7 1.5 1.7 1.5 1.  1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n",
      " 1.3 1.5 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.\n",
      " 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.  4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.\n",
      " 4.9 4.7 4.3 4.4 4.8 5.  4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.\n",
      " 4.4 4.6 4.  3.3 4.2 4.2 4.2 4.3 3.  4.1 6.  5.1 5.9 5.6 5.8 6.6 4.5 6.3\n",
      " 5.8 6.1 5.1 5.3 5.5 5.  5.1 5.3 5.5 6.7 6.9 5.  5.7 4.9 6.7 4.9 5.7 6.\n",
      " 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n",
      " 5.7 5.2 5.  5.2 5.4 5.1]\n",
      "Dimension of the Resulting Array:\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# 1.2) Select the penultimate independent variable\n",
    "\n",
    "penultima_feature = dataset.X[:, -2]\n",
    "dimensao = penultima_feature.shape\n",
    "\n",
    "print(\"Penultimate Independent Variable:\")\n",
    "print(penultima_feature)\n",
    "print(\"Dimension of the Resulting Array:\")\n",
    "print(dimensao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 Samples:\n",
      "[[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Mean for Each Independent Variable/Feature:\n",
      "[6.45 3.03 5.33 2.17]\n"
     ]
    }
   ],
   "source": [
    "#1.3 Select the last 10 samples and calculate the mean for each feature\n",
    "\n",
    "last_10_samples = dataset.X[-10:]\n",
    "mean = last_10_samples.mean(axis=0)\n",
    "\n",
    "print(\"Last 10 Samples:\")\n",
    "print(last_10_samples)\n",
    "print(\"Mean for Each Independent Variable/Feature:\")\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with Values less than or equal to 6 for All Independent Variables:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Number of Samples: 89\n"
     ]
    }
   ],
   "source": [
    "#1.4 Select samples with values less than or equal to 6 for all independent variables\n",
    "\n",
    "selected_samples = dataset.X[(dataset.X <= 6).all(axis=1)]\n",
    "print(\"Samples with Values less than or equal to 6 for All Independent Variables:\")\n",
    "print(selected_samples)\n",
    "num_selected_samples = len(selected_samples)\n",
    "print(\"Number of Samples:\", num_selected_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with a Different Class/Label from 'Iris-setosa':\n",
      "[[7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Number of Samples: 100\n"
     ]
    }
   ],
   "source": [
    "# 1.5) Select samples with a class/label different from 'Irissetosa'\n",
    "\n",
    "different_class_samples = dataset.X[dataset.y != 'Iris-setosa']\n",
    "print(\"Samples with a Different Class/Label from 'Iris-setosa':\")\n",
    "print(different_class_samples)\n",
    "num_different_class_samples = len(different_class_samples)\n",
    "print(\"Number of Samples:\", num_different_class_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use these methods to the script/notebook of Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropna:\n",
      "Dataset shape: (150, 4)\n",
      "\n",
      "After dropna:\n",
      "Dataset shape: (134, 4)\n",
      "\n",
      "Before fillna with 0:\n",
      "        sepal_length  sepal_width  petal_length  petal_width\n",
      "mean        5.839041     3.049655      3.746259     1.213793\n",
      "median      5.800000     3.000000      4.300000     1.300000\n",
      "min         4.300000     2.000000      1.000000     0.100000\n",
      "max         7.900000     4.200000      6.900000     2.500000\n",
      "var         0.697311     0.177534      3.144255     0.575258\n",
      "\n",
      "Filling NaNs with 0:\n",
      "\n",
      "After fillna with 0:\n",
      "        sepal_length  sepal_width  petal_length  petal_width\n",
      "mean        5.683333     2.948000      3.671333     1.173333\n",
      "median      5.700000     3.000000      4.250000     1.300000\n",
      "min         0.000000     0.000000      0.000000     0.000000\n",
      "max         7.900000     4.200000      6.900000     2.500000\n",
      "var         1.563656     0.471296      3.356445     0.603556\n",
      "\n",
      "Before removing sample by index:\n",
      "Dataset shape: (150, 4)\n",
      "\n",
      "After removing sample at index 3:\n",
      "Dataset shape: (149, 4)\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before dropna:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "dataset.dropna()\n",
    "print(\"\\nAfter dropna:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "print(\"\\nBefore fillna with 0:\")\n",
    "print(dataset.summary())\n",
    "print(\"\\nFilling NaNs with 0:\")\n",
    "dataset.fillna(0)\n",
    "print(\"\\nAfter fillna with 0:\")\n",
    "print(dataset.summary())\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "print(\"\\nBefore removing sample by index:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "dataset.remove_by_index(3)\n",
    "print(\"\\nAfter removing sample at index 3:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 2\n",
    "#### Exercise 3: Implementing SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "dataset.X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features with percentile 0:  4\n",
      "Number of features with percentile 25:  1\n",
      "Number of features with percentile 50:  2\n",
      "Number of features with percentile 75:  3\n",
      "Number of features with percentile 100:  4\n"
     ]
    }
   ],
   "source": [
    "selector0 = SelectPercentile(f_classification, percentile = 0)\n",
    "selector25 = SelectPercentile(f_classification, percentile = 0.25)\n",
    "selector50 = SelectPercentile(f_classification, percentile = 0.50)\n",
    "selector75 = SelectPercentile(f_classification, percentile = 0.75)\n",
    "selector1 = SelectPercentile(f_classification, percentile = 1)\n",
    "\n",
    "\n",
    "# do the fit_transform to all selectors and show the len of the features\n",
    "transform0 = selector0.fit_transform(dataset)\n",
    "print(\"Number of features with percentile 0: \", len(transform0.features))\n",
    "\n",
    "\n",
    "transform25 = selector25.fit_transform(dataset)\n",
    "print(\"Number of features with percentile 25: \", len(transform25.features))\n",
    "\n",
    "\n",
    "transform50 = selector50.fit_transform(dataset)\n",
    "print(\"Number of features with percentile 50: \", len(transform50.features))\n",
    "\n",
    "transform75 = selector75.fit_transform(dataset)\n",
    "print(\"Number of features with percentile 75: \", len(transform75.features))\n",
    "\n",
    "transform1 = selector1.fit_transform(dataset)\n",
    "print(\"Number of features with percentile 100: \", len(transform1.features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Exercise 4: Test Manhattan distance \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan funtion = [0 9]\n",
      "sklearn funtion = [[0. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import manhattan_distances as manhattan_distances_sklearn\n",
    "from si.statistics.manhattan_distance import manhattan_distance\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "distance = manhattan_distance(x, y)\n",
    "\n",
    "# sklearn distance\n",
    "\n",
    "sklearn_distance = manhattan_distances_sklearn(x.reshape(1, -1), y)\n",
    "assert np.allclose(distance, sklearn_distance)\n",
    "\n",
    "print(f\"Manhattan funtion = {distance}\")\n",
    "print(f\"sklearn funtion = {sklearn_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Test the PCA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [-0.65653988 -0.72971237  0.1757674   0.07470647]]\n",
      "[[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [ 0.65653988  0.72971237 -0.1757674  -0.07470647]]\n"
     ]
    }
   ],
   "source": [
    "# test PCA function\n",
    "\n",
    "from si.decomposition.pca import PCA\n",
    "from sklearn.decomposition import PCA as PCA_sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(dataset)\n",
    "\n",
    "print(pca.components)\n",
    "\n",
    "# sklearn PCA\n",
    "pca_sklearn = PCA_sklearn(n_components=2)\n",
    "pca_sklearn.fit_transform(dataset.X)\n",
    "print(pca_sklearn.components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 6: Implementing stratified splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stratified split:\n",
      "Train dataset shape: (120, 4)\n",
      "Test dataset shape: (30, 4)\n",
      "\n",
      "\n",
      "\n",
      "After stratified split:\n",
      "Train dataset shape: (120, 4)\n",
      "Test dataset shape: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.model_selection.split import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "print(\"Before stratified split:\")\n",
    "\n",
    "train1, test1 = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(\"Train dataset shape:\", train1.shape())\n",
    "print(\"Test dataset shape:\", test1.shape())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"After stratified split:\")\n",
    "\n",
    "train, test = stratified_train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(\"Train dataset shape:\", train.shape())\n",
    "print(\"Test dataset shape:\", test.shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Implementing the KNNRegressor with RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 3.0\n",
      "RMSE sklearn = 3.0\n"
     ]
    }
   ],
   "source": [
    "### RMSE\n",
    "\n",
    "# test RMSE function\n",
    "\n",
    "from si.metrics.rmse import rmse\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 2, 3])\n",
    "y_pred = np.array([4, 5, 6])\n",
    "rmse_value = rmse(y_true, y_pred)\n",
    "\n",
    "print(f\"RMSE = {rmse_value}\")\n",
    "\n",
    "# compare with sklearn RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse_sklearn = sqrt(mean_squared_error(y_true, y_pred))\n",
    "assert np.allclose(rmse_value, rmse_sklearn)\n",
    "\n",
    "print(f\"RMSE sklearn = {rmse_sklearn}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score RMSE: 81.36259969252635\n"
     ]
    }
   ],
   "source": [
    "### KNN\n",
    "\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.models.knn_regressor import KNNRegressor\n",
    "\n",
    "cpu = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\cpu\\\\cpu.csv\", sep = \",\", features=True, label=True)\n",
    "\n",
    "train, test = train_test_split(cpu, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the KNN_Classifier\n",
    "\n",
    "knn = KNNRegressor(k=3)\n",
    "\n",
    "# fit the model\n",
    "\n",
    "knn.fit(train)\n",
    "\n",
    "# evaluate the model on test data\n",
    "\n",
    "score = knn.score(test)\n",
    "print(\"Score RMSE:\", score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score RMSE sklearn: 81.36021813423898\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### KNN sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "knn_sklearn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_sklearn.fit(train.X, train.y)\n",
    "y_pred = knn_sklearn.predict(test.X)\n",
    "score_sklearn = sqrt(mean_squared_error(test.y, y_pred))\n",
    "\n",
    "print(\"Score RMSE sklearn:\", score_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional – Categorical Naïve-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [0 1 0 1]\n",
      "Classification error: 1.0\n"
     ]
    }
   ],
   "source": [
    "### test categoricalnaivebayes\n",
    "\n",
    "from si.models.categorical_nb import CategoricalNB\n",
    "from si.data.dataset import Dataset\n",
    "\n",
    "\n",
    "X = np.array([[0, 1, 0], [1, 0, 1], [1, 1, 0], [0, 1, 1]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "  \n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "model = CategoricalNB(smoothing=1)\n",
    "model.fit(dataset)\n",
    "\n",
    "predictions = model.predict(dataset)\n",
    "error = model.score(dataset)\n",
    "\n",
    "print(\"Predicted classes:\", predictions)\n",
    "print(\"Classification error:\", error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes sklearn: [0 1 0 1]\n",
      "Classification error sklearn: 1.0\n"
     ]
    }
   ],
   "source": [
    "# test categoricalnaivebayes sklearn with the same dataset\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB as CategoricalNB_sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_sklearn = CategoricalNB_sklearn(alpha=1)\n",
    "model_sklearn.fit(X, y)\n",
    "predictions_sklearn = model_sklearn.predict(X)\n",
    "error_sklearn = accuracy_score(y, predictions_sklearn)\n",
    "\n",
    "print(\"Predicted classes sklearn:\", predictions_sklearn)\n",
    "print(\"Classification error sklearn:\", error_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 5\n",
    "             \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Test RidgeRegression with Least Squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58823529 1.08145743]\n",
      "8.5\n",
      "0.07698961937716277\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with least squares function\n",
    "\n",
    "from si.models.ridge_regression_least_squares import RidgeRegressionLeastSquares\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "# fit the model\n",
    "model = RidgeRegressionLeastSquares()\n",
    "model.fit(dataset_)\n",
    "print(model.theta)\n",
    "print(model.theta_zero)\n",
    "\n",
    "# compute the score\n",
    "print(model.score(dataset_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58823529 1.08145743]\n",
      "8.5\n",
      "0.07698961937716259\n"
     ]
    }
   ],
   "source": [
    "### test ridge regression with least squares\n",
    "\n",
    "from si.models.ridge_regression import RidgeRegression\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.mse import mse\n",
    "from si.model_selection.split import *\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()\n",
    "    # scale data\n",
    "X = (dataset_.X - np.nanmean(dataset_.X, axis=0)) / np.nanstd(dataset_.X, axis=0)\n",
    "model.fit(X, dataset_.y)\n",
    "print(model.coef_) # should be the same as theta\n",
    "print(model.intercept_) # should be the same as theta_zero\n",
    "print(mse(dataset_.y, model.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal_length <= 1.9\n",
      "\tleft: Iris-setosa\n",
      "\tright: petal_length <= 4.7\n",
      "\t  left: petal_width <= 1.5\n",
      "\t    left: Iris-versicolor\n",
      "\t    right: Iris-virginica\n",
      "\t  right: petal_length <= 5.1\n",
      "\t    left: petal_width <= 1.7\n",
      "\t      left: Iris-versicolor\n",
      "\t      right: Iris-virginica\n",
      "\t    right: Iris-virginica\n",
      "Score: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "#use the iris dataset to test the decision tree classifier\n",
    "\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.model_selection.split import *\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "#split the dataset into train and test\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "#train the decision tree classifier using the train dataset\n",
    "\n",
    "model = DecisionTreeClassifier(min_sample_split=3, max_depth=3, mode='entropy')\n",
    "model.fit(train)\n",
    "\n",
    "#print the tree\n",
    "model.print_tree()\n",
    "\n",
    "#evaluate the model on the test dataset\n",
    "\n",
    "score = model.score(test)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9760765550239234\n",
      "Predicted classes: [1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#use the breast cancer dataset to test the voting classifier\n",
    "\n",
    "from si.ensemble.voting_classifier import VotingClassifier\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.model_selection.split import *\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "#split the dataset into train and test\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "lg = LogisticRegression(l2_penalty=1, alpha=0.001, max_iter=1000)\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# initialize the Voting classifier\n",
    "model = VotingClassifier([knn, lg, dt])\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "#evaluate the model on the test dataset\n",
    "\n",
    "score = model.score(test)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "print(\"Predicted classes:\", model.predict(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: Test the RandomForestClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9795918367346939\n",
      "Accuracy score sklearn:  0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "### test random forest classifier\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.models.random_forest_classifier import RandomForestClassifier\n",
    "    \n",
    "# read iris dataset\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "# split dataset into train and test set\n",
    "train_set, test_set = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "# create random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5, min_sample_split=2, mode='gini')\n",
    "\n",
    "# fit classifier to train set\n",
    "clf.fit(train_set)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(test_set)\n",
    "\n",
    "# calculate accuracy score\n",
    "score = accuracy(test_set.y, y_pred)\n",
    "print('Accuracy score: ', score)\n",
    "\n",
    "# compare to sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as skRandomForestClassifier\n",
    "    \n",
    "clf1 = skRandomForestClassifier(n_estimators=10, max_depth=5, min_samples_split=2)\n",
    "clf1.fit(train_set.X, train_set.y)\n",
    "y_pred = clf1.predict(test_set.X)\n",
    "score = accuracy(test_set.y, clf1.predict(test_set.X))\n",
    "print('Accuracy score sklearn: ', score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: Implementing the StackingClassifier ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280575539568345\n"
     ]
    }
   ],
   "source": [
    "# test stacking classifier\n",
    "\n",
    "\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.ensemble.stacking_classifier import StackingClassifier\n",
    "\n",
    "data = read_csv('C:\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv', sep=\",\",features=True,label=True)\n",
    "train, test = stratified_train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "#knnregressor\n",
    "knn = KNNClassifier(k=5)\n",
    "    \n",
    "#logistic regression\n",
    "lr=LogisticRegression(l2_penalty=0.1, alpha=0.1, max_iter=1000)\n",
    "\n",
    "#decisiontreee\n",
    "dt=DecisionTreeClassifier(min_sample_split=2, max_depth=10, mode='gini')\n",
    "\n",
    "#final model\n",
    "final_model=KNNClassifier(k=5)\n",
    "modelos=[knn,lr,dt]\n",
    "exercise=StackingClassifier(modelos,final_model)\n",
    "exercise.fit(train)\n",
    "print(exercise.score(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9784172661870504\n"
     ]
    }
   ],
   "source": [
    "# test stacking classifier sklearn\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier as StackingClassifier_sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#knnregressor\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#logistic regression\n",
    "lr=LogisticRegression(penalty=\"l2\", C=0.1, max_iter=1000)\n",
    "\n",
    "#decisiontreee\n",
    "dt=DecisionTreeClassifier(min_samples_split=2, max_depth=10, criterion='gini')\n",
    "\n",
    "#final model\n",
    "final_model=KNeighborsClassifier(n_neighbors=5)\n",
    "models=[('knn',knn),('lr',lr),('dt',dt)]\n",
    "exercise=StackingClassifier_sklearn(estimators=models,final_estimator=final_model)\n",
    "exercise.fit(train.X, train.y)\n",
    "print(accuracy(test.y, exercise.predict(test.X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test k_fold_cross_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# test k_cross_validation\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.model_selection.cross_validation import k_fold_cross_validation\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# read breast cancer dataset\n",
    "\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# note that in a real application, you should leave a test set aside\n",
    "scores = k_fold_cross_validation(model=lr, dataset=data, cv=5)\n",
    "scores\n",
    "\n",
    "print(f\"Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores):.2f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste grid_search_cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 6000}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test grid search\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.model_selection.cross_validation import k_fold_cross_validation\n",
    "from si.model_selection.grid_search import grid_search_cv\n",
    "\n",
    "# read breast cancer dataset\n",
    "\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "scores = k_fold_cross_validation(lg, data, cv=5)\n",
    "scores\n",
    "\n",
    "lg = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001, 0.00001),\n",
    "    'max_iter': (1000, 2000, 3000, 4000, 5000, 6000)\n",
    "}\n",
    "\n",
    "# cross validate the model\n",
    "scores = grid_search_cv(lg,\n",
    "                        data,\n",
    "                        hyperparameter_grid=parameter_grid,\n",
    "                        cv=3)\n",
    "\n",
    "scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 6000}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "scores['best_hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9683908045977012"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best score\n",
    "scores['best_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11: Test the randomized_search_cv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.45166666666666666, 0.4633333333333333, 0.43166666666666664, 0.4683333333333333, 0.44166666666666665, 0.405, 0.47833333333333333, 0.46166666666666667], 'hyperparameters': [{'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000}], 'best_hyperparameters': {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, 'best_score': 0.47833333333333333}\n",
      "Best hyperparameters: {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}\n",
      "Best score: 0.47833333333333333\n"
     ]
    }
   ],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "num_samples = 600\n",
    "num_features = 100\n",
    "num_classes = 2\n",
    "\n",
    "# random data\n",
    "X = np.random.rand(num_samples, num_features)  \n",
    "y = np.random.randint(0, num_classes, size=num_samples)  # classe aleatórios\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "#  features and class name\n",
    "dataset_.features = [\"feature_\" + str(i) for i in range(num_features)]\n",
    "dataset_.label = \"class_label\"\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid_ = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001),\n",
    "    'max_iter': (1000, 2000)\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "results_ = randomized_search_cv(knn,\n",
    "                              dataset_,\n",
    "                              hyperparameter_grid=parameter_grid_,\n",
    "                              cv=3,\n",
    "                              n_iter=8)\n",
    "\n",
    "# print the results\n",
    "print(results_)\n",
    "\n",
    "#get the best hyperparameters\n",
    "\n",
    "best_hyperparameters = results_['best_hyperparameters']\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "\n",
    "# get the best score\n",
    "best_score = results_['best_score']\n",
    "print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'hyperparameters': [{'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}], 'best_hyperparameters': {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, 'best_score': 0.0}\n",
      "Best hyperparameters: {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}\n",
      "Best score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test using not random data\n",
    "\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "\n",
    "parameter_grid_ = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001),\n",
    "    'max_iter': (1000, 2000)\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "\n",
    "results_ = randomized_search_cv(knn,\n",
    "                                dataset_,\n",
    "                                hyperparameter_grid=parameter_grid_,\n",
    "                                cv=3,\n",
    "                                n_iter=8)\n",
    "\n",
    "# print the results\n",
    "\n",
    "print(results_)\n",
    "\n",
    "#get the best hyperparameters\n",
    "\n",
    "best_hyperparameters = results_['best_hyperparameters']\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\t\n",
    "\n",
    "best_score = results_['best_score']\n",
    "print(f\"Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.        , 0.01413933, 0.00033569, 0.01266567, 0.        ,\n",
      "       0.01599773, 0.00033259, 0.0166111 ]), 'std_fit_time': array([0.        , 0.00229707, 0.00047474, 0.00169879, 0.        ,\n",
      "       0.00283002, 0.00047036, 0.00255495]), 'mean_score_time': array([0.        , 0.00033315, 0.        , 0.        , 0.        ,\n",
      "       0.00066733, 0.        , 0.00033331]), 'std_score_time': array([0.        , 0.00047115, 0.        , 0.        , 0.        ,\n",
      "       0.00047188, 0.        , 0.00047137]), 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[1000, 1000, 2000, 2000, 1000, 1000, 2000, 2000],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_C': masked_array(data=[1, 1, 1, 1, 10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'penalty': 'l1', 'max_iter': 1000, 'C': 1}, {'penalty': 'l2', 'max_iter': 1000, 'C': 1}, {'penalty': 'l1', 'max_iter': 2000, 'C': 1}, {'penalty': 'l2', 'max_iter': 2000, 'C': 1}, {'penalty': 'l1', 'max_iter': 1000, 'C': 10}, {'penalty': 'l2', 'max_iter': 1000, 'C': 10}, {'penalty': 'l1', 'max_iter': 2000, 'C': 10}, {'penalty': 'l2', 'max_iter': 2000, 'C': 10}], 'split0_test_score': array([ nan, 0.5 ,  nan, 0.5 ,  nan, 0.49,  nan, 0.49]), 'split1_test_score': array([  nan, 0.525,   nan, 0.525,   nan, 0.52 ,   nan, 0.52 ]), 'split2_test_score': array([  nan, 0.475,   nan, 0.475,   nan, 0.475,   nan, 0.475]), 'mean_test_score': array([  nan, 0.5  ,   nan, 0.5  ,   nan, 0.495,   nan, 0.495]), 'std_test_score': array([       nan, 0.02041241,        nan, 0.02041241,        nan,\n",
      "       0.01870829,        nan, 0.01870829]), 'rank_test_score': array([5, 1, 6, 1, 7, 3, 8, 3])}\n",
      "{'penalty': 'l2', 'max_iter': 1000, 'C': 1}\n",
      "0.5\n",
      "LogisticRegression(C=1, max_iter=1000)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joana\\anaconda3\\envs\\si\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\joana\\anaconda3\\envs\\si\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [  nan 0.5     nan 0.5     nan 0.495   nan 0.495]\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# test randomized search cv sklearn\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegression_sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 600\n",
    "num_features = 100\n",
    "num_classes = 2\n",
    "\n",
    "# random data\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = np.random.randint(0, num_classes, size=num_samples)  # classe aleatórios\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "#  features and class name\n",
    "dataset_.features = [\"feature_\" + str(i) for i in range(num_features)]\n",
    "dataset_.label = \"class_label\"\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression_sklearn()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid_ = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10],\n",
    "    'max_iter': [1000, 2000]\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "\n",
    "model = RandomizedSearchCV(knn, parameter_grid_, cv=3, n_iter=8)\n",
    "model.fit(X, y)\n",
    "\n",
    "# print the results\n",
    "print(model.cv_results_)\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)\n",
    "print(model.best_estimator_)\n",
    "print(model.best_index_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Dropout layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n",
      "output training: [[0. 8. 0. 2. 0. 0. 0. 0. 8. 8. 0. 4. 8. 4. 0.]]\n",
      "mask: [[0. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 2. 2. 2. 0.]]\n",
      "output inference: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n"
     ]
    }
   ],
   "source": [
    "# Test the layer with a random input \n",
    "\n",
    "from si.neural_networks.layers import DropoutLayer\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import si\n",
    "#test the dropout layer\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(5, size=(1, 15))\n",
    "print(\"x:\", x)\n",
    "# create the dropout layer\n",
    "dropout_layer = DropoutLayer(0.5) #he dropout layer applies a dropout probability of 0.5\n",
    "# test the dropout layer\n",
    "print(\"output training:\", dropout_layer.forward_propagation(x, training=True))\n",
    "# print the mask\n",
    "print(\"mask:\", dropout_layer.mask)\n",
    "print(\"output inference:\", dropout_layer.forward_propagation(x, training=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Test TanhActivation and SoftmaxActivation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[5 8 9 5 0 0 1 7 6 9]]\n",
      "\n",
      "output: [[0.9999092  0.99999977 0.99999997 0.9999092  0.         0.\n",
      "  0.76159416 0.99999834 0.99998771 0.99999997]]\n",
      "\n",
      "derivative: [[1.81583231e-04 4.50140598e-07 6.09199171e-08 1.81583231e-04\n",
      "  1.00000000e+00 1.00000000e+00 4.19974342e-01 3.32610934e-06\n",
      "  2.45765474e-05 6.09199171e-08]]\n",
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the tanh activation function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from si.neural_networks.activation import TanhActivation\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(10, size=(1, 10))\n",
    "print(\"x:\", x)\n",
    "print()\n",
    "\n",
    "tanh_activation = TanhActivation()\n",
    "\n",
    "print(\"output:\", tanh_activation.forward_propagation(x, training=True))\n",
    "print()\n",
    "print(\"derivative:\", tanh_activation.derivative(x))\n",
    "print('\\n', '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[5 8 9 5 0 0 1 7 6 9]]\n",
      "\n",
      "output: [[7.07108730e-03 1.42026585e-01 3.86068285e-01 7.07108730e-03\n",
      "  4.76446115e-05 4.76446115e-05 1.29511482e-04 5.22486607e-02\n",
      "  1.92212081e-02 3.86068285e-01]]\n",
      "\n",
      "derivative: [[7.02108703e-03 1.21855034e-01 2.37019564e-01 7.02108703e-03\n",
      "  4.76423415e-05 4.76423415e-05 1.29494708e-04 4.95187382e-02\n",
      "  1.88517533e-02 2.37019564e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the softmax activation function\n",
    "\n",
    "import numpy as np\n",
    "from si.neural_networks.activation import SoftmaxActivation\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(10, size=(1, 10))\n",
    "print(\"x:\", x)\n",
    "print()\n",
    "\n",
    "softmax_activation = SoftmaxActivation()\n",
    "\n",
    "print(\"output:\", softmax_activation.forward_propagation(x, training=True))\n",
    "print()\n",
    "print(\"derivative:\", softmax_activation.derivative(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14: Test CategoricalCrossEntropy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.353878387381596\n",
      "\n",
      "[[ -0.          -1.05263158  -0.        ]\n",
      " [ -0.          -0.         -10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# categorical cross entropy loss function\n",
    "\n",
    "from si.neural_networks.losses import CategoricalCrossEntropy\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "loss = CategoricalCrossEntropy()\n",
    "print(loss.loss(y_true, y_pred))\n",
    "print()\n",
    "print(loss.derivative(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15: Test Adam class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999 1.999 2.999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from si.neural_networks.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "\n",
    "w = np.array([1, 2, 3])\n",
    "grad_loss_w = np.array([1, 2, 3])\n",
    "\n",
    "print(adam.update(w, grad_loss_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16 : Build, train and evaluate a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de features: 32\n"
     ]
    }
   ],
   "source": [
    "# The training dataset has 32 features\n",
    "\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(32)\n",
    "X = np.random.rand(400, 32)\n",
    "y = np.random.randint(2, size=(400, 1))\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "print('Número de features:', dataset.shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train dataset: (320, 32)\n",
      "Shape of the test dataset: (80, 32)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and test\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "#shape of the train and test datasets\n",
    "\n",
    "print('Shape of the train dataset:', train_dataset.shape())\n",
    "print('Shape of the test dataset:', test_dataset.shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - loss: 220.1206 - accuracy: 0.5781\n",
      "Epoch 2/100 - loss: 219.3871 - accuracy: 0.5656\n",
      "Epoch 3/100 - loss: 217.4933 - accuracy: 0.6000\n",
      "Epoch 4/100 - loss: 216.2198 - accuracy: 0.5687\n",
      "Epoch 5/100 - loss: 216.0575 - accuracy: 0.5813\n",
      "Epoch 6/100 - loss: 215.9487 - accuracy: 0.5594\n",
      "Epoch 7/100 - loss: 214.6500 - accuracy: 0.5719\n",
      "Epoch 8/100 - loss: 211.2856 - accuracy: 0.6094\n",
      "Epoch 9/100 - loss: 208.7744 - accuracy: 0.6094\n",
      "Epoch 10/100 - loss: 205.4479 - accuracy: 0.6031\n",
      "Epoch 11/100 - loss: 204.5813 - accuracy: 0.6656\n",
      "Epoch 12/100 - loss: 204.3691 - accuracy: 0.6500\n",
      "Epoch 13/100 - loss: 198.6170 - accuracy: 0.6281\n",
      "Epoch 14/100 - loss: 193.2537 - accuracy: 0.6719\n",
      "Epoch 15/100 - loss: 195.6992 - accuracy: 0.6438\n",
      "Epoch 16/100 - loss: 193.3270 - accuracy: 0.6469\n",
      "Epoch 17/100 - loss: 194.9498 - accuracy: 0.6312\n",
      "Epoch 18/100 - loss: 187.0420 - accuracy: 0.6937\n",
      "Epoch 19/100 - loss: 188.5031 - accuracy: 0.6844\n",
      "Epoch 20/100 - loss: 189.8811 - accuracy: 0.6844\n",
      "Epoch 21/100 - loss: 179.9737 - accuracy: 0.7094\n",
      "Epoch 22/100 - loss: 188.5762 - accuracy: 0.6781\n",
      "Epoch 23/100 - loss: 185.3326 - accuracy: 0.7000\n",
      "Epoch 24/100 - loss: 174.0848 - accuracy: 0.7188\n",
      "Epoch 25/100 - loss: 175.5910 - accuracy: 0.7063\n",
      "Epoch 26/100 - loss: 174.7146 - accuracy: 0.7219\n",
      "Epoch 27/100 - loss: 173.2654 - accuracy: 0.7344\n",
      "Epoch 28/100 - loss: 173.4872 - accuracy: 0.7312\n",
      "Epoch 29/100 - loss: 168.4682 - accuracy: 0.7156\n",
      "Epoch 30/100 - loss: 164.0328 - accuracy: 0.7344\n",
      "Epoch 31/100 - loss: 157.4788 - accuracy: 0.7531\n",
      "Epoch 32/100 - loss: 163.9624 - accuracy: 0.7594\n",
      "Epoch 33/100 - loss: 157.7212 - accuracy: 0.7469\n",
      "Epoch 34/100 - loss: 155.5562 - accuracy: 0.7906\n",
      "Epoch 35/100 - loss: 164.3712 - accuracy: 0.7250\n",
      "Epoch 36/100 - loss: 145.6408 - accuracy: 0.7875\n",
      "Epoch 37/100 - loss: 132.2268 - accuracy: 0.8281\n",
      "Epoch 38/100 - loss: 154.5135 - accuracy: 0.7781\n",
      "Epoch 39/100 - loss: 147.2490 - accuracy: 0.7688\n",
      "Epoch 40/100 - loss: 124.7287 - accuracy: 0.8344\n",
      "Epoch 41/100 - loss: 151.0142 - accuracy: 0.7844\n",
      "Epoch 42/100 - loss: 128.2472 - accuracy: 0.8281\n",
      "Epoch 43/100 - loss: 151.4647 - accuracy: 0.7594\n",
      "Epoch 44/100 - loss: 122.1735 - accuracy: 0.8281\n",
      "Epoch 45/100 - loss: 133.1040 - accuracy: 0.8250\n",
      "Epoch 46/100 - loss: 123.0160 - accuracy: 0.8156\n",
      "Epoch 47/100 - loss: 100.6098 - accuracy: 0.8750\n",
      "Epoch 48/100 - loss: 105.8559 - accuracy: 0.8719\n",
      "Epoch 49/100 - loss: 150.3348 - accuracy: 0.7875\n",
      "Epoch 50/100 - loss: 99.8124 - accuracy: 0.8906\n",
      "Epoch 51/100 - loss: 114.4015 - accuracy: 0.8469\n",
      "Epoch 52/100 - loss: 102.1527 - accuracy: 0.8719\n",
      "Epoch 53/100 - loss: 91.1101 - accuracy: 0.8969\n",
      "Epoch 54/100 - loss: 83.7596 - accuracy: 0.8812\n",
      "Epoch 55/100 - loss: 75.5356 - accuracy: 0.9219\n",
      "Epoch 56/100 - loss: 72.1756 - accuracy: 0.9250\n",
      "Epoch 57/100 - loss: 87.0161 - accuracy: 0.8906\n",
      "Epoch 58/100 - loss: 58.2656 - accuracy: 0.9531\n",
      "Epoch 59/100 - loss: 55.5004 - accuracy: 0.9469\n",
      "Epoch 60/100 - loss: 72.7994 - accuracy: 0.9187\n",
      "Epoch 61/100 - loss: 94.7002 - accuracy: 0.8719\n",
      "Epoch 62/100 - loss: 58.0675 - accuracy: 0.9344\n",
      "Epoch 63/100 - loss: 51.6702 - accuracy: 0.9563\n",
      "Epoch 64/100 - loss: 66.0394 - accuracy: 0.9156\n",
      "Epoch 65/100 - loss: 111.4243 - accuracy: 0.8656\n",
      "Epoch 66/100 - loss: 42.0872 - accuracy: 0.9656\n",
      "Epoch 67/100 - loss: 39.0066 - accuracy: 0.9750\n",
      "Epoch 68/100 - loss: 33.8257 - accuracy: 0.9750\n",
      "Epoch 69/100 - loss: 31.7601 - accuracy: 0.9812\n",
      "Epoch 70/100 - loss: 28.7386 - accuracy: 0.9875\n",
      "Epoch 71/100 - loss: 25.9758 - accuracy: 0.9844\n",
      "Epoch 72/100 - loss: 22.5184 - accuracy: 0.9906\n",
      "Epoch 73/100 - loss: 20.1942 - accuracy: 0.9875\n",
      "Epoch 74/100 - loss: 19.9451 - accuracy: 0.9938\n",
      "Epoch 75/100 - loss: 16.9674 - accuracy: 0.9969\n",
      "Epoch 76/100 - loss: 48.7027 - accuracy: 0.9531\n",
      "Epoch 77/100 - loss: 23.9685 - accuracy: 0.9875\n",
      "Epoch 78/100 - loss: 17.2146 - accuracy: 0.9938\n",
      "Epoch 79/100 - loss: 14.5819 - accuracy: 0.9969\n",
      "Epoch 80/100 - loss: 12.0226 - accuracy: 0.9969\n",
      "Epoch 81/100 - loss: 10.9095 - accuracy: 0.9969\n",
      "Epoch 82/100 - loss: 10.5209 - accuracy: 0.9969\n",
      "Epoch 83/100 - loss: 9.2129 - accuracy: 0.9969\n",
      "Epoch 84/100 - loss: 8.6340 - accuracy: 0.9969\n",
      "Epoch 85/100 - loss: 8.2086 - accuracy: 0.9969\n",
      "Epoch 86/100 - loss: 8.6589 - accuracy: 0.9969\n",
      "Epoch 87/100 - loss: 7.4132 - accuracy: 0.9969\n",
      "Epoch 88/100 - loss: 6.9709 - accuracy: 0.9969\n",
      "Epoch 89/100 - loss: 7.1837 - accuracy: 0.9969\n",
      "Epoch 90/100 - loss: 6.2164 - accuracy: 0.9969\n",
      "Epoch 91/100 - loss: 6.0099 - accuracy: 0.9969\n",
      "Epoch 92/100 - loss: 5.8117 - accuracy: 0.9969\n",
      "Epoch 93/100 - loss: 5.5398 - accuracy: 0.9969\n",
      "Epoch 94/100 - loss: 5.3867 - accuracy: 0.9969\n",
      "Epoch 95/100 - loss: 5.0586 - accuracy: 0.9969\n",
      "Epoch 96/100 - loss: 5.5889 - accuracy: 0.9969\n",
      "Epoch 97/100 - loss: 5.0600 - accuracy: 0.9969\n",
      "Epoch 98/100 - loss: 4.7475 - accuracy: 0.9969\n",
      "Epoch 99/100 - loss: 4.5771 - accuracy: 0.9969\n",
      "Epoch 100/100 - loss: 4.5105 - accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<si.neural_networks.neural_network.NeuralNetwork at 0x1c6d5b74648>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "from si.neural_networks.layers import Layer, DenseLayer, DropoutLayer\n",
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "from si.neural_networks.optimizers import SGD\n",
    "from si.neural_networks.activation import ReLUActivation,SigmoidActivation\n",
    "from si.neural_networks.losses import BinaryCrossEntropy\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "\n",
    "\n",
    "# Train the NN for 100 epochs, with batchsize of 16 with a learning rate of 0.01. \n",
    "# Use the SGD optimizer, \n",
    "# use the BinaryCrossEntropy loss function \n",
    "# use accuracy metric. \n",
    "# Use the ReLU activation function for the hidden layers \n",
    "# the Sigmoid activation function for the output layer.\n",
    "\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(epochs=100, batch_size=16, optimizer=SGD, learning_rate=0.01, verbose=True, loss=BinaryCrossEntropy, metric=accuracy)\n",
    "nn.add(DenseLayer(32, (dataset.shape()[1],)))\n",
    "nn.add(ReLUActivation())\n",
    "nn.add(DenseLayer(16))\n",
    "nn.add(ReLUActivation())\n",
    "nn.add(DenseLayer(1))\n",
    "nn.add(SigmoidActivation())\n",
    "nn.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3875\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "print(nn.score(test_dataset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

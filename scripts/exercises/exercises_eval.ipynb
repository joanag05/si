{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook : Exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from si.data.dataset import Dataset\n",
    "from si.feature_selection.select_percentile import SelectPercentile\n",
    "from si.statistics.f_classification import f_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Class 1\n",
    "\n",
    "## Exercise 1: NumPy array Indexing/Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCICIO 1\n",
    "\n",
    "#1.1 - Loading the \"iris.csv\" using the appropriate method\n",
    "\n",
    "path = r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris.csv'\n",
    "\n",
    "dataset = read_csv(path,features=True, label=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penultimate Independent Variable:\n",
      "[1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n",
      " 1.7 1.5 1.7 1.5 1.  1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n",
      " 1.3 1.5 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.\n",
      " 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.  4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.\n",
      " 4.9 4.7 4.3 4.4 4.8 5.  4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.\n",
      " 4.4 4.6 4.  3.3 4.2 4.2 4.2 4.3 3.  4.1 6.  5.1 5.9 5.6 5.8 6.6 4.5 6.3\n",
      " 5.8 6.1 5.1 5.3 5.5 5.  5.1 5.3 5.5 6.7 6.9 5.  5.7 4.9 6.7 4.9 5.7 6.\n",
      " 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n",
      " 5.7 5.2 5.  5.2 5.4 5.1]\n",
      "Dimension of the Resulting Array:\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# 1.2) Select the penultimate independent variable\n",
    "\n",
    "penultima_feature = dataset.X[:, -2]\n",
    "dimensao = penultima_feature.shape\n",
    "\n",
    "print(\"Penultimate Independent Variable:\")\n",
    "print(penultima_feature)\n",
    "print(\"Dimension of the Resulting Array:\")\n",
    "print(dimensao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 Samples:\n",
      "[[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Mean for Each Independent Variable/Feature:\n",
      "[6.45 3.03 5.33 2.17]\n"
     ]
    }
   ],
   "source": [
    "#1.3 Select the last 10 samples and calculate the mean for each feature\n",
    "\n",
    "last_10_samples = dataset.X[-10:]\n",
    "mean = last_10_samples.mean(axis=0)\n",
    "\n",
    "print(\"Last 10 Samples:\")\n",
    "print(last_10_samples)\n",
    "print(\"Mean for Each Independent Variable/Feature:\")\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with Values less than or equal to 6 for All Independent Variables:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Number of Samples: 89\n"
     ]
    }
   ],
   "source": [
    "#1.4 Select samples with values less than or equal to 6 for all independent variables\n",
    "\n",
    "selected_samples = dataset.X[(dataset.X <= 6).all(axis=1)]\n",
    "print(\"Samples with Values less than or equal to 6 for All Independent Variables:\")\n",
    "print(selected_samples)\n",
    "num_selected_samples = len(selected_samples)\n",
    "print(\"Number of Samples:\", num_selected_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with a Different Class/Label from 'Iris-setosa':\n",
      "[[7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Number of Samples: 100\n"
     ]
    }
   ],
   "source": [
    "# 1.5) Select samples with a class/label different from 'Irissetosa'\n",
    "\n",
    "different_class_samples = dataset.X[dataset.y != 'Iris-setosa']\n",
    "print(\"Samples with a Different Class/Label from 'Iris-setosa':\")\n",
    "print(different_class_samples)\n",
    "num_different_class_samples = len(different_class_samples)\n",
    "print(\"Number of Samples:\", num_different_class_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use these methods to the script/notebook of Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropna:\n",
      "Dataset shape: (150, 4)\n",
      "\n",
      "After dropna:\n",
      "Dataset shape: (134, 4)\n",
      "\n",
      "Before fillna with 0:\n",
      "        sepal_length  sepal_width  petal_length  petal_width\n",
      "mean        5.839041     3.049655      3.746259     1.213793\n",
      "median      5.800000     3.000000      4.300000     1.300000\n",
      "min         4.300000     2.000000      1.000000     0.100000\n",
      "max         7.900000     4.200000      6.900000     2.500000\n",
      "var         0.697311     0.177534      3.144255     0.575258\n",
      "\n",
      "Filling NaNs with 0:\n",
      "\n",
      "After fillna with 0:\n",
      "        sepal_length  sepal_width  petal_length  petal_width\n",
      "mean        5.683333     2.948000      3.671333     1.173333\n",
      "median      5.700000     3.000000      4.250000     1.300000\n",
      "min         0.000000     0.000000      0.000000     0.000000\n",
      "max         7.900000     4.200000      6.900000     2.500000\n",
      "var         1.563656     0.471296      3.356445     0.603556\n",
      "\n",
      "Before removing sample by index:\n",
      "Dataset shape: (150, 4)\n",
      "\n",
      "After removing sample at index 3:\n",
      "Dataset shape: (149, 4)\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before dropna:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "dataset.dropna()\n",
    "print(\"\\nAfter dropna:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "print(\"\\nBefore fillna with 0:\")\n",
    "print(dataset.summary())\n",
    "print(\"\\nFilling NaNs with 0:\")\n",
    "dataset.fillna(0)\n",
    "print(\"\\nAfter fillna with 0:\")\n",
    "print(dataset.summary())\n",
    "\n",
    "# read dataset\n",
    "dataset = read_csv(r'C:\\Users\\joana\\OneDrive\\Documentos\\GitHub\\si\\datasets\\iris\\iris_missing_data.csv', features=True, label=True)\n",
    "\n",
    "print(\"\\nBefore removing sample by index:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "dataset.remove_by_index(3)\n",
    "print(\"\\nAfter removing sample at index 3:\")\n",
    "print(\"Dataset shape:\", dataset.shape())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2\n",
    "## Exercise 3: Test SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "dataset.X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with percentile 0:  ['sepal_width', 'sepal_length', 'petal_width', 'petal_length']\n",
      "Number of features with percentile 0:  4\n",
      "\n",
      "Features with percentile 25:  ['petal_length']\n",
      "Number of features with percentile 25:  1\n",
      "\n",
      "Features with percentile 50:  ['petal_width', 'petal_length']\n",
      "Number of features with percentile 50:  2\n",
      "\n",
      "Features with percentile 75:  ['sepal_length', 'petal_width', 'petal_length']\n",
      "Number of features with percentile 75:  3\n",
      "\n",
      "Features with percentile 100:  ['sepal_width', 'sepal_length', 'petal_width', 'petal_length']\n",
      "Number of features with percentile 100:  4\n"
     ]
    }
   ],
   "source": [
    "selector0 = SelectPercentile(f_classification, percentile = 0)\n",
    "selector25 = SelectPercentile(f_classification, percentile = 0.25)\n",
    "selector50 = SelectPercentile(f_classification, percentile = 0.50)\n",
    "selector75 = SelectPercentile(f_classification, percentile = 0.75)\n",
    "selector1 = SelectPercentile(f_classification, percentile = 1)\n",
    "\n",
    "\n",
    "# do the fit_transform to all selectors and show the len of the features\n",
    "transform0 = selector0.fit_transform(dataset)\n",
    "print(\"Features with percentile 0: \", transform0.features)\n",
    "print(\"Number of features with percentile 0: \", len(transform0.features))\n",
    "\n",
    "print()\n",
    "\n",
    "transform25 = selector25.fit_transform(dataset)\n",
    "print(\"Features with percentile 25: \", transform25.features)\n",
    "print(\"Number of features with percentile 25: \", len(transform25.features))\n",
    "\n",
    "print()\n",
    "\n",
    "transform50 = selector50.fit_transform(dataset)\n",
    "print(\"Features with percentile 50: \", transform50.features)\n",
    "print(\"Number of features with percentile 50: \", len(transform50.features))\n",
    "\n",
    "print()\n",
    "\n",
    "transform75 = selector75.fit_transform(dataset)\n",
    "print(\"Features with percentile 75: \", transform75.features)\n",
    "print(\"Number of features with percentile 75: \", len(transform75.features))\n",
    "\n",
    "print()\n",
    "\n",
    "transform1 = selector1.fit_transform(dataset)\n",
    "print(\"Features with percentile 100: \", transform1.features)\n",
    "print(\"Number of features with percentile 100: \", len(transform1.features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Exercise 4: Test Manhattan distance \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan funtion = [0 9]\n",
      "sklearn funtion = [[0. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import manhattan_distances as manhattan_distances_sklearn\n",
    "from si.statistics.manhattan_distance import manhattan_distance\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "distance = manhattan_distance(x, y)\n",
    "\n",
    "# sklearn distance\n",
    "\n",
    "sklearn_distance = manhattan_distances_sklearn(x.reshape(1, -1), y)\n",
    "assert np.allclose(distance, sklearn_distance)\n",
    "\n",
    "print(f\"Manhattan funtion = {distance}\")\n",
    "print(f\"sklearn funtion = {sklearn_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Test the PCA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: [[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [-0.65653988 -0.72971237  0.1757674   0.07470647]]\n",
      "\n",
      "PCA Sklearn: [[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [ 0.65653988  0.72971237 -0.1757674  -0.07470647]]\n"
     ]
    }
   ],
   "source": [
    "# test PCA function\n",
    "\n",
    "from si.decomposition.pca import PCA\n",
    "from sklearn.decomposition import PCA as PCA_sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(dataset)\n",
    "\n",
    "print('PCA:', pca.components)\n",
    "\n",
    "print()\n",
    "\n",
    "# sklearn PCA\n",
    "pca_sklearn = PCA_sklearn(n_components=2)\n",
    "pca_sklearn.fit_transform(dataset.X)\n",
    "print('PCA Sklearn:', pca_sklearn.components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 6: Implementing stratified splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stratified split:\n",
      "Train dataset shape: (120, 4)\n",
      "Test dataset shape: (30, 4)\n",
      "\n",
      "\n",
      "\n",
      "After stratified split:\n",
      "Train dataset shape: (120, 4)\n",
      "Test dataset shape: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.model_selection.split import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "print(\"Before stratified split:\")\n",
    "\n",
    "train1, test1 = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(\"Train dataset shape:\", train1.shape())\n",
    "print(\"Test dataset shape:\", test1.shape())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"After stratified split:\")\n",
    "\n",
    "train, test = stratified_train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(\"Train dataset shape:\", train.shape())\n",
    "print(\"Test dataset shape:\", test.shape())\n",
    "\n",
    "# test with an example\n",
    "\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.model_selection.split import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 2.0\n",
      "RMSE sklearn = 2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test RMSE function\n",
    "\n",
    "from si.metrics.rmse import rmse\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([6, 5, 8])\n",
    "y_pred = np.array([4, 3, 6])\n",
    "rmse_value = rmse(y_true, y_pred)\n",
    "\n",
    "print(f\"RMSE = {rmse_value}\")\n",
    "\n",
    "# compare with sklearn RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse_sklearn = sqrt(mean_squared_error(y_true, y_pred))\n",
    "assert np.allclose(rmse_value, rmse_sklearn)\n",
    "\n",
    "print(f\"RMSE sklearn = {rmse_sklearn}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste KNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score RMSE: 81.36259969252635\n",
      "Score RMSE sklearn: 81.36021813423898\n"
     ]
    }
   ],
   "source": [
    "### KNN\n",
    "\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.models.knn_regressor import KNNRegressor\n",
    "\n",
    "cpu = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\cpu\\\\cpu.csv\", sep = \",\", features=True, label=True)\n",
    "\n",
    "train, test = train_test_split(cpu, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the KNN_Classifier\n",
    "\n",
    "knn = KNNRegressor(k=3)\n",
    "\n",
    "# fit the model\n",
    "\n",
    "knn.fit(train)\n",
    "\n",
    "# evaluate the model on test data\n",
    "\n",
    "score = knn.score(test)\n",
    "print(\"Score RMSE:\", score)\n",
    "\n",
    "# compare with sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "knn_sklearn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_sklearn.fit(train.X, train.y)\n",
    "y_pred = knn_sklearn.predict(test.X)\n",
    "score_sklearn = sqrt(mean_squared_error(test.y, y_pred))\n",
    "print(\"Score RMSE sklearn:\", score_sklearn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional – Categorical Naïve-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [0 1 0 1]\n",
      "Classification error: 1.0\n",
      "Predicted classes sklearn: [0 1 0 1]\n",
      "Classification error sklearn: 1.0\n"
     ]
    }
   ],
   "source": [
    "### test categoricalnaivebayes\n",
    "\n",
    "from si.models.categorical_nb import CategoricalNB\n",
    "from si.data.dataset import Dataset\n",
    "\n",
    "\n",
    "X = np.array([[0, 1, 0], [1, 0, 1], [1, 1, 0], [0, 1, 1]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "  \n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "model = CategoricalNB(smoothing=1)\n",
    "model.fit(dataset)\n",
    "\n",
    "predictions = model.predict(dataset)\n",
    "error = model.score(dataset)\n",
    "\n",
    "print(\"Predicted classes:\", predictions)\n",
    "print(\"Classification error:\", error)\n",
    "\n",
    "# compare with sklearn\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB as CategoricalNB_sklearn\n",
    "\n",
    "model_sklearn = CategoricalNB_sklearn(alpha=1)\n",
    "model_sklearn.fit(X, y)\n",
    "predictions_sklearn = model_sklearn.predict(X)\n",
    "error_sklearn = model_sklearn.score(X, y)\n",
    "\n",
    "print(\"Predicted classes sklearn:\", predictions_sklearn)\n",
    "print(\"Classification error sklearn:\", error_sklearn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 5\n",
    "             \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Test RidgeRegression with Least Squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.58823529 1.08145743]\n",
      "Theta Zero: 8.5\n",
      "Score: 0.07698961937716277\n",
      "\n",
      "Theta Sklearn: [0.58823529 1.08145743]\n",
      "Theta Zeri Sklearn: 8.5\n",
      "Score: 0.07698961937716259\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with least squares function\n",
    "\n",
    "from si.models.ridge_regression_least_squares import RidgeRegressionLeastSquares\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "# fit the model\n",
    "model = RidgeRegressionLeastSquares()\n",
    "model.fit(dataset_)\n",
    "print(\"Theta:\", model.theta)\n",
    "print(\"Theta Zero:\", model.theta_zero)\n",
    "\n",
    "# compute the score\n",
    "print('Score:',model.score(dataset_))\n",
    "\n",
    "print()\n",
    "### test ridge regression with least squares\n",
    "\n",
    "from si.models.ridge_regression import RidgeRegression\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.mse import mse\n",
    "from si.model_selection.split import *\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()\n",
    "    # scale data\n",
    "X = (dataset_.X - np.nanmean(dataset_.X, axis=0)) / np.nanstd(dataset_.X, axis=0)\n",
    "model.fit(X, dataset_.y)\n",
    "print('Theta Sklearn:', model.coef_) # should be the same as theta\n",
    "print('Theta Zeri Sklearn:', model.intercept_) # should be the same as theta_zero\n",
    "print('Score:', mse(dataset_.y, model.predict(X))) # should be the same as score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal_length <= 1.9\n",
      "\tleft: Iris-setosa\n",
      "\tright: petal_length <= 4.7\n",
      "\t  left: petal_width <= 1.5\n",
      "\t    left: Iris-versicolor\n",
      "\t    right: Iris-virginica\n",
      "\t  right: petal_length <= 5.1\n",
      "\t    left: petal_width <= 1.7\n",
      "\t      left: Iris-versicolor\n",
      "\t      right: Iris-virginica\n",
      "\t    right: Iris-virginica\n",
      "Score: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "#use the iris dataset to test the decision tree classifier\n",
    "\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.model_selection.split import *\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "#split the dataset into train and test\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "#train the decision tree classifier using the train dataset\n",
    "\n",
    "model = DecisionTreeClassifier(min_sample_split=3, max_depth=3, mode='entropy')\n",
    "model.fit(train)\n",
    "\n",
    "#print the tree\n",
    "model.print_tree()\n",
    "\n",
    "#evaluate the model on the test dataset\n",
    "\n",
    "score = model.score(test)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9760765550239234\n",
      "Predicted classes: [1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#use the breast cancer dataset to test the voting classifier\n",
    "\n",
    "from si.ensemble.voting_classifier import VotingClassifier\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.model_selection.split import *\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "\n",
    "filename = \"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\"\n",
    "dataset = read_csv(filename= filename, sep = ',', features = True, label = True)\n",
    "\n",
    "#split the dataset into train and test\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "lg = LogisticRegression(l2_penalty=1, alpha=0.001, max_iter=1000)\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# initialize the Voting classifier\n",
    "model = VotingClassifier([knn, lg, dt])\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "#evaluate the model on the test dataset\n",
    "\n",
    "score = model.score(test)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "print(\"Predicted classes:\", model.predict(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2: Test the RandomForestClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9795918367346939\n",
      "Accuracy score sklearn:  0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "### test random forest classifier\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.models.random_forest_classifier import RandomForestClassifier\n",
    "    \n",
    "# read iris dataset\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\iris\\\\iris.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "# split dataset into train and test set\n",
    "train_set, test_set = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "# create random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5, min_sample_split=2, mode='gini')\n",
    "\n",
    "# fit classifier to train set\n",
    "clf.fit(train_set)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(test_set)\n",
    "\n",
    "# calculate accuracy score\n",
    "score = accuracy(test_set.y, y_pred)\n",
    "print('Accuracy score: ', score)\n",
    "\n",
    "# compare to sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as skRandomForestClassifier\n",
    "    \n",
    "clf1 = skRandomForestClassifier(n_estimators=10, max_depth=5, min_samples_split=2)\n",
    "clf1.fit(train_set.X, train_set.y)\n",
    "y_pred = clf1.predict(test_set.X)\n",
    "score = accuracy(test_set.y, clf1.predict(test_set.X))\n",
    "print('Accuracy score sklearn: ', score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Implementing the StackingClassifier ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9784172661870504\n",
      "Accuracy Sklearn: 0.9784172661870504\n"
     ]
    }
   ],
   "source": [
    "# test stacking classifier\n",
    "\n",
    "\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.ensemble.stacking_classifier import StackingClassifier\n",
    "\n",
    "data = read_csv('C:\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv', sep=\",\",features=True,label=True)\n",
    "train, test = stratified_train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "data = \"C:\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\"  \n",
    "breast=read_csv(data, sep=\",\",features=True,label=True)\n",
    "train_data, test_data = stratified_train_test_split(breast, test_size=0.20, random_state=42)\n",
    "\n",
    "#knnregressor\n",
    "knn = KNNClassifier(k=3)\n",
    "    \n",
    "#logistic regression\n",
    "LG=LogisticRegression(l2_penalty=1, alpha=0.001, max_iter=1000)\n",
    "\n",
    "#decisiontreee\n",
    "DT=DecisionTreeClassifier(min_sample_split=3, max_depth=3, mode='gini')\n",
    "\n",
    "# Final Model (Choose a different model as the final model)\n",
    "final_model = LogisticRegression(l2_penalty=1, alpha=0.001, max_iter=1000)\n",
    "\n",
    "# Initialize StackingClassifier\n",
    "models = [knn, LG, DT]\n",
    "exercise = StackingClassifier(models, final_model)\n",
    "exercise.fit(train_data)\n",
    "print('Accuracy:', exercise.score(test_data))\n",
    "\n",
    "# compare with sklearn\n",
    "from sklearn.ensemble import StackingClassifier as SklearnStackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Logistic Regression\n",
    "LG = LogisticRegression(penalty='l2', C=1, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier(min_samples_split=3, max_depth=3, criterion='gini')\n",
    "\n",
    "# Final Model (Choose a different model as the final model)\n",
    "final_model = LogisticRegression(penalty='l2', C=1, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Initialize StackingClassifier\n",
    "models = [('knn', knn), ('LG', LG), ('DT', DT)]\n",
    "sklearn = SklearnStackingClassifier(estimators=models, final_estimator=final_model)\n",
    "sklearn.fit(train_data.X, train_data.y)\n",
    "y_pred = sklearn.predict(test_data.X)\n",
    "print('Accuracy Sklearn:', accuracy_score(test_data.y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test k_fold_cross_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# test k_cross_validation\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.model_selection.cross_validation import k_fold_cross_validation\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# read breast cancer dataset\n",
    "\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# note that in a real application, you should leave a test set aside\n",
    "scores = k_fold_cross_validation(model=lr, dataset=data, cv=5)\n",
    "scores\n",
    "\n",
    "print(f\"Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores):.2f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste grid_search_cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': [0.9669540229885057,\n",
       "  0.9655172413793104,\n",
       "  0.9669540229885056,\n",
       "  0.9655172413793104,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9655172413793104,\n",
       "  0.9655172413793104,\n",
       "  0.9655172413793104,\n",
       "  0.9669540229885057,\n",
       "  0.9640804597701149,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885056,\n",
       "  0.9669540229885057,\n",
       "  0.9655172413793104,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885056,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885057,\n",
       "  0.9669540229885056,\n",
       "  0.9669540229885057],\n",
       " 'hyperparameters': [{'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 2000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 3000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 4000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 5000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 6000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 2000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 3000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 4000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 5000},\n",
       "  {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 6000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 1000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 2000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 3000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 4000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 5000},\n",
       "  {'l2_penalty': 1, 'alpha': 1e-05, 'max_iter': 6000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 2000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 3000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 4000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 5000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 6000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 1000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 3000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 4000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 5000},\n",
       "  {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 6000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 1000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 2000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 3000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 4000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 5000},\n",
       "  {'l2_penalty': 10, 'alpha': 1e-05, 'max_iter': 6000}],\n",
       " 'best_hyperparameters': {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000},\n",
       " 'best_score': 0.9669540229885057}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test grid search\n",
    "\n",
    "from si.io.csv_file import read_csv\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.model_selection.cross_validation import k_fold_cross_validation\n",
    "from si.model_selection.grid_search import grid_search_cv\n",
    "\n",
    "# read breast cancer dataset\n",
    "\n",
    "data = read_csv(\"\\\\Users\\\\joana\\\\OneDrive\\\\Documentos\\\\GitHub\\\\si\\\\datasets\\\\breast_bin\\\\breast-bin.csv\", sep=',', features=True, label=True)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "scores = k_fold_cross_validation(lg, data, cv=5)\n",
    "scores\n",
    "\n",
    "lg = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001, 0.00001),\n",
    "    'max_iter': (1000, 2000, 3000, 4000, 5000, 6000)\n",
    "}\n",
    "\n",
    "# cross validate the model\n",
    "scores = grid_search_cv(lg,\n",
    "                        data,\n",
    "                        hyperparameter_grid=parameter_grid,\n",
    "                        cv=3)\n",
    "\n",
    "scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "scores['best_hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9669540229885057"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best score\n",
    "scores['best_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: Test the randomized_search_cv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.5116666666666667, 0.5083333333333334, 0.5316666666666667, 0.49, 0.5283333333333333, 0.505, 0.52, 0.5283333333333333], 'hyperparameters': [{'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 2000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 2000}], 'best_hyperparameters': {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, 'best_score': 0.5316666666666667}\n",
      "Best hyperparameters: {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}\n",
      "Best score: 0.5316666666666667\n"
     ]
    }
   ],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "num_samples = 600\n",
    "num_features = 100\n",
    "num_classes = 2\n",
    "\n",
    "# random data\n",
    "X = np.random.rand(num_samples, num_features)  \n",
    "y = np.random.randint(0, num_classes, size=num_samples)  # classe aleatórios\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "#  features and class name\n",
    "dataset_.features = [\"feature_\" + str(i) for i in range(num_features)]\n",
    "dataset_.label = \"class_label\"\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid_ = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001),\n",
    "    'max_iter': (1000, 2000)\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "results_ = randomized_search_cv(knn,\n",
    "                              dataset_,\n",
    "                              hyperparameter_grid=parameter_grid_,\n",
    "                              cv=3,\n",
    "                              n_iter=8)\n",
    "\n",
    "# print the results\n",
    "print(results_)\n",
    "\n",
    "#get the best hyperparameters\n",
    "\n",
    "best_hyperparameters = results_['best_hyperparameters']\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "\n",
    "# get the best score\n",
    "best_score = results_['best_score']\n",
    "print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'hyperparameters': [{'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 2000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 1, 'alpha': 0.0001, 'max_iter': 2000}, {'l2_penalty': 10, 'alpha': 0.0001, 'max_iter': 1000}, {'l2_penalty': 10, 'alpha': 0.001, 'max_iter': 1000}], 'best_hyperparameters': {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}, 'best_score': 0.0}\n",
      "Best hyperparameters: {'l2_penalty': 1, 'alpha': 0.001, 'max_iter': 1000}\n",
      "Best score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test using not random data\n",
    "\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "\n",
    "parameter_grid_ = {\n",
    "    'l2_penalty': (1, 10),\n",
    "    'alpha': (0.001, 0.0001),\n",
    "    'max_iter': (1000, 2000)\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "\n",
    "results_ = randomized_search_cv(knn,\n",
    "                                dataset_,\n",
    "                                hyperparameter_grid=parameter_grid_,\n",
    "                                cv=3,\n",
    "                                n_iter=8)\n",
    "\n",
    "# print the results\n",
    "\n",
    "print(results_)\n",
    "\n",
    "#get the best hyperparameters\n",
    "\n",
    "best_hyperparameters = results_['best_hyperparameters']\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\t\n",
    "\n",
    "best_score = results_['best_score']\n",
    "print(f\"Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.        , 0.0246218 , 0.        , 0.00886099, 0.00033363,\n",
      "       0.0126617 , 0.0003322 , 0.01408633]), 'std_fit_time': array([0.        , 0.01646289, 0.        , 0.00199649, 0.00047182,\n",
      "       0.00170477, 0.0004698 , 0.00175666]), 'mean_score_time': array([0.        , 0.00066622, 0.        , 0.00029802, 0.        ,\n",
      "       0.00066678, 0.        , 0.00033458]), 'std_score_time': array([0.        , 0.00047109, 0.        , 0.00042147, 0.        ,\n",
      "       0.00047148, 0.        , 0.00047317]), 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[1000, 1000, 2000, 2000, 1000, 1000, 2000, 2000],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_C': masked_array(data=[1, 1, 1, 1, 10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'penalty': 'l1', 'max_iter': 1000, 'C': 1}, {'penalty': 'l2', 'max_iter': 1000, 'C': 1}, {'penalty': 'l1', 'max_iter': 2000, 'C': 1}, {'penalty': 'l2', 'max_iter': 2000, 'C': 1}, {'penalty': 'l1', 'max_iter': 1000, 'C': 10}, {'penalty': 'l2', 'max_iter': 1000, 'C': 10}, {'penalty': 'l1', 'max_iter': 2000, 'C': 10}, {'penalty': 'l2', 'max_iter': 2000, 'C': 10}], 'split0_test_score': array([ nan, 0.48,  nan, 0.48,  nan, 0.49,  nan, 0.49]), 'split1_test_score': array([ nan, 0.52,  nan, 0.52,  nan, 0.52,  nan, 0.52]), 'split2_test_score': array([  nan, 0.49 ,   nan, 0.49 ,   nan, 0.505,   nan, 0.505]), 'mean_test_score': array([       nan, 0.49666667,        nan, 0.49666667,        nan,\n",
      "       0.505     ,        nan, 0.505     ]), 'std_test_score': array([       nan, 0.01699673,        nan, 0.01699673,        nan,\n",
      "       0.01224745,        nan, 0.01224745]), 'rank_test_score': array([5, 3, 5, 3, 5, 1, 5, 1])}\n",
      "{'penalty': 'l2', 'max_iter': 1000, 'C': 10}\n",
      "0.505\n",
      "LogisticRegression(C=10, max_iter=1000)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\joana\\anaconda3\\envs\\si_portefolio\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.49666667        nan 0.49666667        nan 0.505\n",
      "        nan 0.505     ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test randomized search cv sklearn\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegression_sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 600\n",
    "num_features = 100\n",
    "num_classes = 2\n",
    "\n",
    "# random data\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = np.random.randint(0, num_classes, size=num_samples)  # classe aleatórios\n",
    "\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "#  features and class name\n",
    "dataset_.features = [\"feature_\" + str(i) for i in range(num_features)]\n",
    "dataset_.label = \"class_label\"\n",
    "\n",
    "# initialize the Logistic Regression model\n",
    "knn = LogisticRegression_sklearn()\n",
    "\n",
    "# parameter grid\n",
    "parameter_grid_ = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10],\n",
    "    'max_iter': [1000, 2000]\n",
    "    }\n",
    "\n",
    "# cross validate the model\n",
    "\n",
    "model = RandomizedSearchCV(knn, parameter_grid_, cv=3, n_iter=8)\n",
    "model.fit(X, y)\n",
    "\n",
    "# print the results\n",
    "print(model.cv_results_)\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)\n",
    "print(model.best_estimator_)\n",
    "print(model.best_index_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12: Dropout layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n",
      "output training: [[0. 8. 0. 2. 0. 0. 0. 0. 8. 8. 0. 4. 8. 4. 0.]]\n",
      "mask: [[0 1 0 1 0 1 0 0 1 1 0 1 1 1 0]]\n",
      "output inference: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n"
     ]
    }
   ],
   "source": [
    "# Test the layer with a random input \n",
    "\n",
    "from si.neural_networks.layers import DropoutLayer\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(5, size=(1, 15))\n",
    "print(\"x:\", x)\n",
    "# create the dropout layer\n",
    "dropout_layer = DropoutLayer(0.5) #he dropout layer applies a dropout probability of 0.5\n",
    "# test the dropout layer\n",
    "print(\"output training:\", dropout_layer.forward_propagation(x, training=True))\n",
    "# print the mask\n",
    "print(\"mask:\", dropout_layer.mask)\n",
    "print(\"output inference:\", dropout_layer.forward_propagation(x, training=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13: Test TanhActivation and SoftmaxActivation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n",
      "\n",
      "output: [[0.99505475 0.9993293  0.         0.76159416 0.99505475 0.\n",
      "  0.         0.76159416 0.9993293  0.9993293  0.76159416 0.96402758\n",
      "  0.9993293  0.96402758 0.9993293 ]]\n",
      "\n",
      "derivative: [[0.00986604 0.00134095 1.         0.41997434 0.00986604 1.\n",
      "  1.         0.41997434 0.00134095 0.00134095 0.41997434 0.07065082\n",
      "  0.00134095 0.07065082 0.00134095]]\n",
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the tanh activation function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from si.neural_networks.activation import TanhActivation\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(5, size=(1, 15))\n",
    "print(\"x:\", x)\n",
    "print()\n",
    "\n",
    "tanh_activation = TanhActivation()\n",
    "\n",
    "print(\"output:\", tanh_activation.forward_propagation(x, training=True))\n",
    "print()\n",
    "print(\"derivative:\", tanh_activation.derivative(x))\n",
    "print('\\n', '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[3 4 0 1 3 0 0 1 4 4 1 2 4 2 4]]\n",
      "\n",
      "output: [[0.05923281 0.16101147 0.00294903 0.00801629 0.05923281 0.00294903\n",
      "  0.00294903 0.00801629 0.16101147 0.16101147 0.00801629 0.02179053\n",
      "  0.16101147 0.02179053 0.16101147]]\n",
      "\n",
      "derivative: [[0.05572428 0.13508678 0.00294033 0.00795203 0.05572428 0.00294033\n",
      "  0.00294033 0.00795203 0.13508678 0.13508678 0.00795203 0.02131571\n",
      "  0.13508678 0.02131571 0.13508678]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the softmax activation function\n",
    "\n",
    "import numpy as np\n",
    "from si.neural_networks.activation import SoftmaxActivation\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(5, size=(1, 15))\n",
    "print(\"x:\", x)\n",
    "print()\n",
    "\n",
    "softmax_activation = SoftmaxActivation()\n",
    "\n",
    "print(\"output:\", softmax_activation.forward_propagation(x, training=True))\n",
    "print()\n",
    "print(\"derivative:\", softmax_activation.derivative(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14: Test CategoricalCrossEntropy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.26053659 0.28329719 0.28690184 ... 0.50884161 0.00125448 0.29467725]\n",
      " [0.11084116 0.70554298 0.43903072 ... 0.50949214 0.76749905 0.7950342 ]\n",
      " [0.87926686 0.33284972 0.15252811 ... 0.55162697 0.17750792 0.32603568]\n",
      " ...\n",
      " [0.88055144 0.15765703 0.16756591 ... 0.69642906 0.25893001 0.14851551]\n",
      " [0.73518418 0.83286755 0.97058767 ... 0.43583335 0.24905149 0.09716275]\n",
      " [0.35600837 0.7413659  0.73061723 ... 0.93447737 0.15197847 0.68075503]]\n",
      "\n",
      "Target: [1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
      " 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1\n",
      " 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
      " 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
      " 0 0 1 0 1 1 1 1]\n",
      "\n",
      "Output: 2.353878387381596\n",
      "\n",
      "Gradiente: [[ -0.          -1.05263158  -0.        ]\n",
      " [ -0.          -0.         -10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# categorical cross entropy loss function\n",
    "\n",
    "from si.neural_networks.losses import CategoricalCrossEntropy\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "loss = CategoricalCrossEntropy()\n",
    "\n",
    "print('Input:', X)\n",
    "print()\n",
    "print('Target:', y)\n",
    "print()\n",
    "print('Output:', loss.loss(y_true, y_pred))\n",
    "print()\n",
    "print('Gradiente:', loss.derivative(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 15: Test Adam class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.26053659 0.28329719 0.28690184 ... 0.50884161 0.00125448 0.29467725]\n",
      " [0.11084116 0.70554298 0.43903072 ... 0.50949214 0.76749905 0.7950342 ]\n",
      " [0.87926686 0.33284972 0.15252811 ... 0.55162697 0.17750792 0.32603568]\n",
      " ...\n",
      " [0.88055144 0.15765703 0.16756591 ... 0.69642906 0.25893001 0.14851551]\n",
      " [0.73518418 0.83286755 0.97058767 ... 0.43583335 0.24905149 0.09716275]\n",
      " [0.35600837 0.7413659  0.73061723 ... 0.93447737 0.15197847 0.68075503]]\n",
      "\n",
      "Target: [1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
      " 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1\n",
      " 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
      " 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
      " 0 0 1 0 1 1 1 1]\n",
      "\n",
      "Output: [0.999 1.999 2.999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from si.neural_networks.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "w = np.array([1, 2, 3])\n",
    "grad_loss_w = np.array([1, 2, 3])\n",
    "\n",
    "print('Input:', X)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Target:', y)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Output:', adam.update(w, grad_loss_w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[3 4 2 4 4]]\n",
      "\n",
      "Target: [0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      "\n",
      "Output: [[2.999 3.999 1.999 3.999 3.999]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 5, size=(1, 5))\n",
    "\n",
    "np.random.seed(42)\n",
    "y = np.random.random(5)\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "print(f'Target: {y}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {adam.update(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16 : Build, train and evaluate a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de features: 32\n"
     ]
    }
   ],
   "source": [
    "# The training dataset has 32 features\n",
    "\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(32)\n",
    "X = np.random.rand(400, 32)\n",
    "y = np.random.randint(2, size=(400, 1))\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "print('Número de features:', dataset.shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train dataset: (321, 32)\n",
      "Shape of the test dataset: (79, 32)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and test\n",
    "\n",
    "train_dataset, test_dataset = stratified_train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "#shape of the train and test datasets\n",
    "\n",
    "print('Shape of the train dataset:', train_dataset.shape())\n",
    "print('Shape of the test dataset:', test_dataset.shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4936708860759494\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "import numpy as np\n",
    "from si.neural_networks.layers import Layer, DenseLayer, DropoutLayer\n",
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "from si.neural_networks.optimizers import SGD\n",
    "from si.neural_networks.activation import ReLUActivation,SigmoidActivation\n",
    "from si.neural_networks.losses import BinaryCrossEntropy\n",
    "from si.model_selection.split import stratified_train_test_split\n",
    "from si.metrics.accuracy import accuracy\n",
    "\n",
    "\n",
    "# Train the NN for 100 epochs, with batchsize of 16 with a learning rate of 0.01. \n",
    "# Use the SGD optimizer, \n",
    "# use the BinaryCrossEntropy loss function \n",
    "# use accuracy metric. \n",
    "# Use the ReLU activation function for the hidden layers \n",
    "# the Sigmoid activation function for the output layer.\n",
    "\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(epochs=1000, batch_size=16, optimizer=SGD, learning_rate=0.01, verbose=False, loss=BinaryCrossEntropy, metric=accuracy)\n",
    "nn.add(DenseLayer(32, (dataset.shape()[1],)))\n",
    "nn.add(ReLUActivation())\n",
    "nn.add(DenseLayer(16))\n",
    "nn.add(ReLUActivation())\n",
    "nn.add(DenseLayer(1))\n",
    "nn.add(SigmoidActivation())\n",
    "nn.fit(train_dataset)\n",
    "print(nn.score(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 2.4006 - accuracy: 0.4937\n",
      "Score: 0.49367088079452515\n"
     ]
    }
   ],
   "source": [
    "# do the same but with keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=dataset.shape()[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                loss=BinaryCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset.X, train_dataset.y, epochs=1000, batch_size=16, verbose=0)\n",
    "\n",
    "score = model.evaluate(test_dataset.X, test_dataset.y, batch_size=16)\n",
    "print(\"Score:\", score[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
